# -*- coding: utf-8 -*-
"""assignments_logistic-regression_bak-full.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FQny3kMI-Qq34DZlL09hhbwm5z3QKk_U
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as stats
from statsmodels.stats.proportion import proportions_ztest
from sklearn.preprocessing import LabelEncoder
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score , roc_auc_score , roc_curve , classification_report , confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
import os
from copy import deepcopy
import warnings
warnings.filterwarnings ( "ignore" )

data = pd.read_csv("C:/Users/Masum/Downloads/bank-full.csv",sep=';')
data.head()

categorical = data.select_dtypes ( include = "object" )
print ( "Categorical columns:" )
c = 1
for i in categorical.columns:
    print ( c , "." , i )
    c += 1

numerical = data.select_dtypes ( exclude = "object" )
print ( "Numerical columns:" )
c = 1
for i in numerical.columns:
    print ( c , "." , i )
    c += 1

#Dimension of the Data
print ( "Number of Rows:" , data.shape [ 0 ] )
print ( "Number of Columns:" , data.shape [ 1 ] )

data.info ( )

numerical.describe ( )

categorical.describe ( )

#Checking the null values
data.isnull ( ).sum ( )

"""Exploratory Data Analysis"""

#Exploratory Data Analysis
for i in categorical.columns:
    print ( "Column:" , i )
    print ( data [ i ].value_counts ( ) )
    f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
    sns.countplot ( data [ i ] )
    plt.xticks ( rotation = 45 )
    plt.show ( )
    print ( "\n" )

"""Univariate Analysis for Numerical columns using Distribution plot and checking the skewness"""

#Univariate Analysis for Numerical columns using Distribution plot and checking the skewness
for i in numerical.columns:
    print ( "Column:" , i )
    print ( "Skewness for {} is {}".format ( i , round ( data [ i ].skew ( ) , 3 ) ) )
    sns.distplot ( data [ i ] )
    plt.show ( )

"""Bivariate Analysis for Categorical and Numerical columns using Boxen plot, Box plot and Violin plot"""

#Bivariate Analysis for Categorical and Numerical columns using Boxen plot, Box plot and Violin plot
f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.boxenplot ( data [ "poutcome" ] , data [ "pdays" ] )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.violinplot ( data [ "poutcome" ] , data [ "age" ] )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.boxenplot ( data [ "month" ] , data [ "day" ] )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.boxplot ( data [ "contact" ] , data [ "age" ] )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.violinplot ( data [ "loan" ] , data [ "age" ] )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.boxenplot ( data [ "job" ] , data [ "balance" ] )
plt.xticks ( rotation = 45 )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.boxplot ( data [ "default" ] , data [ "age" ] )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.boxplot ( data [ "education" ] , data [ "age" ] )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.boxenplot ( data [ "job" ] , data [ "age" ] )
plt.xticks ( rotation = 45 )
plt.show ( )

"""Bivariate Analysis for Numerical and Numerical columns using scatterplot and using Categorical columns as hue for a better understanding"""

#Bivariate Analysis for Numerical and Numerical columns using scatterplot and using Categorical columns as hue for a better understanding
f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.scatterplot ( data [ "age" ] , data [ "balance" ] , hue = data [ "job" ] )
plt.show ( )

f , ax = plt.subplots ( figsize = ( 7 , 5 ) )
sns.scatterplot ( data [ "age" ] , data [ "campaign" ] , hue = data [ "marital" ] )
plt.show ( )

"""Statistical Modeling
Test of relationship using Chi-square test to check the associations between categorical variables
"""

cat1 , cat2 = [ ] , [ ]
for i in categorical.columns:
    if data [ i ].nunique ( ) == 2:
        cat1.append ( i )
    else:
        cat2.append ( i )

for i in cat2:
    for j in cat1:
        print ( "Chi-square test for {} and {}".format ( i , j ) )
        print ( "Hypothesis testing for association between {} and {}".format ( i , j ) )
        print ( "Ha : There is an association between {} and {}".format ( i , j ) )
        print ( "Ha : There is no association between {} and {}".format ( i , j ) )
        pvalue = round ( stats.chi2_contingency ( pd.crosstab ( data [ i ] , data [ j ] ) ) [ 1 ] , 5 )
        print ( "p-value is" , pvalue )
        if pvalue < 0.05:
            print ( "We reject Ho since p-value of {} is less than alpha of 0.05.".format ( pvalue ) )
            print ( "Hence, we conclude by accepting alternate hypothesis." )
            print ( "Ha : There is an association between {} and {}".format ( i , j ) )
        else:
            print ( "We fail to reject Ho since p-value of {} is greater than alpha of 0.05.".format ( pvalue ) )
            print ( "Hence, we conclude by accepting null hypothesis." )
            print ( "Ho : There is no association between {} and {}".format ( i , j ) )
        print ( "\n" )

"""Test of Proportions using Z-Proportions test"""

for i in range ( len ( cat1 ) ):
    for j in range ( i , len ( cat1 ) ):
        if i != j:
            print ( "Z-proportions test for {} and {}".format ( cat1 [ i ] , cat1 [ j ] ) )
            print ( "Hypothesis testing for proportions between {} and {}".format ( cat1 [ i ] , cat1 [ j ] ) )
            print ( "Ha : The proportion of people opting for Yes in {j} and not {i} is not equal to thehe proportion of people opting for Yes in {j} and {i}".format ( i = cat1 [ i ] , j = cat1 [ j ] ) )
            print ( "Ho : The proportion of people opting for Yes in {j} and not {i} is equal to the proportion of people opting for Yes in {j} and {i}".format ( i = cat1 [ i ] , j = cat1 [ j ] ) )
            c =  pd.crosstab ( data [ cat1 [ i ] ]  , data [ cat1 [ j ] ] )
            print ( c )
            pvalue = round ( proportions_ztest ( [ c [ "yes" ] [ 0 ] , c [ "yes" ] [ 1 ] ] ,
                                      [ c [ "yes" ] [ 0 ] + c [ "no" ] [ 0 ] ,
                                      c [ "yes" ] [ 1 ] + c [ "no" ] [ 1 ] ] ) [ 1 ] , 5 )
            print (  "p-value is" , pvalue )
            if pvalue < 0.05:
                print ( "We reject Ho since p-value of {} is less than alpha of 0.05.".format ( pvalue ) )
                print ( "Hence, we conclude by accepting alternate hypothesis." )
                print ( "The proportion of people opting for Yes in {j} and not {i} is not equal to the proportion of people opting for Yes in {j} and {i}".format ( i = cat1 [ i ] , j = cat1 [ j ] ) )
            else:
                print ( "We fail to reject Ho since p-value of {} is greater than alpha of 0.05.".format ( pvalue ) )
                print ( "Hence, we conclude by accepting null hypothesis." )
                print ( "The proportion of people opting for Yes in {j} and not {i} is equal to the proportion of people opting for Yes in {j} and {i}".format ( i = cat1 [ i ] , j = cat1 [ j ] ) )
            print ( "\n" )

"""Testing for skewness and best transformations"""

#Testing for skewness and best transformations
log , sqrt = [ ] , [ ]
for i in numerical.columns:
    print ( "Checking skewness for" , i )
    a = round ( data [ i ].skew ( ) , 3 )
    print ( "1. Normal Distribution" , a )
    d = pd.DataFrame ( )
    d [ "log" ] = np.log ( data [ i ] )
    b = round ( d [ "log" ].skew ( ) , 3 )
    print ( "2. Log Transformation" , b )
    d [ "Sqrt" ] = np.sqrt ( data [ i ] )
    c = round ( d [ "Sqrt" ].skew ( ) , 3 )
    print ( "3. Sqrt Transformation" , c )
    print ( "The best transformation would be:" )
    if np.abs ( a ) < np.abs ( b ) and np.abs ( a ) < np.abs ( c ):
        print ( "Normal" , a )
    elif np.abs ( b ) < np.abs ( a ) and np.abs ( b ) < np.abs ( c ):
        print ( "Log Transformation" , b )
        log.append ( i )
    else:
        print ( "Sqrt Transformation" , c )
        sqrt.append ( i )
    print ( "\n" )
print ( "Features to undergo Log transformation" , log )
print ( "Features to undergo Sqrt transformation" , sqrt )

"""Checking the correlations between numerical variables"""

#Checking the correlations between numerical variables
f , ax = plt.subplots ( figsize = ( 9 , 7 ) )
sns.heatmap ( numerical.corr ( ) , cmap = "YlGnBu" , annot = True )
plt.show ( )

"""Checking VIF to drop individual predictors based on the inflation value"""

#Checking VIF to drop individual predictors based on the inflation value
vif = [ variance_inflation_factor ( numerical.values , i ) for i in range ( numerical.shape [ 1 ] ) ]
vif = pd.DataFrame ( { "VIF" : vif } , index = numerical.columns )
vif

#I won't be transforming balance and pdays because it would end up creating null values
sqrt.remove ( "balance" )
sqrt.remove ( "pdays" )
for i in log:
    data [ i ] = np.log ( data [ i ] )
for i in sqrt:
    data [ i ] = np.sqrt ( data [ i ] )

#Using Label Encoder for Categorical variables
for i in categorical.columns:
    le = LabelEncoder ( )
    data [ i ] = le.fit_transform ( data [ i ] )

#Splitting the data into X and y
X = data.drop ( "y" , axis = 1 )
y = data [ "y" ]

#Logit Model for the data
xc = sm.add_constant( X )
model = sm.Logit ( y , xc ).fit ( )
model.summary ( )

X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 37 )

smote = SMOTE ( )
X_train_sm , y_train_sm = smote.fit_sample ( X_train , y_train.ravel ( ) )

"""Logistic Regression"""

#Logistic Regression
lr = LogisticRegression ( )
lr.fit ( X_train , y_train )
lr_pred_train = lr.predict ( X_train )
lr_pred_test = lr.predict ( X_test )
cm = confusion_matrix ( y_test , lr_pred_test )
model.append ( "Logistic Regression" )
train.append ( round ( accuracy_score ( y_train , lr_pred_train ) , 3 ) )
test.append ( round ( accuracy_score ( y_test , lr_pred_test ) , 3 ) )
type1.append ( cm [ 0 , 1 ] )
type2.append ( cm [ 1 , 0 ] )
correct.append ( cm [ 0 , 0 ] + cm [ 1 , 1 ] )
errors.append ( cm [ 1 , 0 ] + cm [ 0 , 1 ] )
sens.append ( round ( cm [ 1 , 1 ] / ( cm [ 1 , 1 ] + cm [ 1 , 0 ] ) , 3 ) )
spec.append ( round ( cm [ 0 , 0 ] / ( cm [ 0 , 0 ] + cm [ 0 , 1 ] ) , 3 ) )
ROC.append ( round ( roc_auc_score ( y_test , lr.predict_proba ( X_test ) [ : , 1 ] ) , 3 ) )
print ( "Train score:" , round ( accuracy_score ( y_train , lr_pred_train ) , 3 ) )
print ( "Test score:" , round ( accuracy_score ( y_test , lr_pred_test ) , 3 ) )
print ( "Test Classification Report:\n" , classification_report ( y_test , lr_pred_test ) )
print ( "Test Confusion Matrix:\n" , cm )

Train score: 0.893
Test score: 0.891
Test Classification Report:
               precision    recall  f1-score   support

           0       0.91      0.97      0.94     12005
           1       0.55      0.26      0.35      1559

    accuracy                           0.89     13564
   macro avg       0.73      0.61      0.65     13564
weighted avg       0.87      0.89      0.87     13564

Test Confusion Matrix:
 [[11685   320]
 [ 1160   399]]